From 3e934db2a2570a80e1f525d8910ffd60e59f0a90 Mon Sep 17 00:00:00 2001
From: shu1qin9 <shu1qin9@gmail.com>
Date: Fri, 26 Dec 2025 16:20:09 +0800
Subject: [PATCH] =?UTF-8?q?vLLM:=20=E5=85=A8=E9=9D=A2=E5=90=AF=E7=94=A8=20?=
 =?UTF-8?q?Pascal=EF=BC=88CC=206.x=EF=BC=89=E6=94=AF=E6=8C=81?=
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

- CMakeLists.txt: 精简架构列表，仅保留 6.0/6.1，避免高版本编译器过滤 Pascal
- moe_wna16.cu: 为 CC < 700 补充 __half 原子加实现，保证 MOE WNA16 内核可在 Pascal 正常跑通
- prefix_prefill.py: 同步关闭 sm_70 以下限制，允许 6.x 设备进入前缀预填充路径

至此，vLLM 0.14 可在 Compute Capability 6.0/6.1 显卡完整编译运行。
---
 csrc/moe/moe_wna16.cu | 24 ++++++++++++++++++++++++
 1 file changed, 24 insertions(+)

diff --git a/csrc/moe/moe_wna16.cu b/csrc/moe/moe_wna16.cu
index 7b6a111c0..f750dbab2 100644
--- a/csrc/moe/moe_wna16.cu
+++ b/csrc/moe/moe_wna16.cu
@@ -10,6 +10,30 @@
 
 #define DIVIDE(x, size) (((x) + (size) - 1) / (size))
 
+// Sources: https://github.com/torch/cutorch/blob/master/lib/THC/THCAtomics.cuh#L96C1-L119C7
+// and https://docs.nvidia.com/cuda/cuda-c-programming-guide/#atomicadd
+// and a number of other similar implementations found online.
+
+#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ < 700
+static inline  __device__ void atomicAdd(half *address, half val) {
+	unsigned int * address_as_ui =
+		(unsigned int *) ((char *)address - ((size_t)address & 2));
+	unsigned int old = *address_as_ui;
+	unsigned int assumed;
+
+	do {
+		assumed = old;
+		__half_raw hsum;
+		hsum.x = (size_t)address & 2 ? (old >> 16) : (old & 0xffff);
+		half tmpres = __hadd(hsum, val);
+		hsum = __half_raw(tmpres);
+		old = (size_t)address & 2 ? (old & 0xffff) | (hsum.x << 16) : (old & 0xffff0000) | hsum.x;
+		old = atomicCAS(address_as_ui, assumed, old);
+	} while (assumed != old);
+
+}
+#endif
+
 template <typename scalar_t, int bit, int GROUPS>
 __global__ void moe_wna16_gemm_kernel(
     const scalar_t* __restrict__ input, scalar_t* __restrict__ output,
-- 
2.43.0

